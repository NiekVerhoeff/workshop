{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODOlaZ1rwKgmzDQWWjmvDT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiekVerhoeff/workshop/blob/main/chat_with_zip_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chat with your files - HuggingFace**\n",
        "\n",
        "This notebook demonstrates a Retreival Augmented Generation system that can read data from different files zipped in a zip-file.\n",
        "\n",
        "This notebook requires that the runtimetype is set to T4. To do this, go to the top right corner and click on the downfacing arrow next to the runtime details. Click on Change Runtimetype (Runtimetype wijzigen), choose T4 GPU and save.\n",
        "\n",
        "This notebook was made with help from the following article: https://sabeerali.medium.com/build-your-personal-rag-chatbot-chat-freely-with-your-data-powered-by-llamaindex-and-open-llms-63eb8ad1a053"
      ],
      "metadata": {
        "id": "8fryvHjQBNhZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzgSNC9lnVTc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Initialize things\n",
        "\n",
        "!pip install llama-index\n",
        "!pip install docx2txt\n",
        "#!pip install torch transformers python-pptx Pillow\n",
        "%pip install llama-index-readers-web\n",
        "%pip install llama-index-llms-huggingface\n",
        "#!pip install \"transformers[torch]\" \"huggingface_hub[inference]\"\n",
        "!pip install transformers accelerate pypdf einops bitsandbytes\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "#from llama_index.llms import HuggingFaceLLM\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from typing import Dict\n",
        "from typing import Any\n",
        "from llama_index.core.extractors import PydanticProgramExtractor\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from google.colab import userdata\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from typing import List, Optional\n",
        "\n",
        "from llama_index.llms.huggingface import (\n",
        "    HuggingFaceInferenceAPI,\n",
        "    HuggingFaceLLM,\n",
        ")\n",
        "#HF_TOKEN = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up model\n",
        "\n",
        "#@markdown Choose any Text Generation model from Huggingface: https://huggingface.co/models?pipeline_tag=text-generation&sort=trending\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "system_prompt = \"\"\"<|SYSTEM|>#\n",
        "At your service\n",
        "\"\"\"\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
        "\n",
        "import torch\n",
        "\n",
        "#!pip install accelerate\n",
        "#!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "#from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "\n",
        "#model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "#quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=200.0)\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_id, device_map = \"auto\", quantization_config=quantization_config)\n",
        "\n",
        "#quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=200.0)\n",
        "\n",
        "model = \"mistralai/Mistral-7B-v0.1\" #@param {type:\"string\"}\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=model,\n",
        "    model_name=model,\n",
        "    device_map=\"auto\",\n",
        "    tokenizer_kwargs={\"max_length\": 4096},\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"llm_int8_enable_fp32_cpu_offload\": True,\n",
        "        \"bnb_4bit_quant_type\": 'nf4',\n",
        "        \"bnb_4bit_use_double_quant\":True,\n",
        "        \"bnb_4bit_compute_dtype\":torch.bfloat16,\n",
        "        \"load_in_8bit\": True}\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Qppt1fmppjTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload a zip-file with documents\n",
        "#@markdown supported extensions are listed here: https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload the ZIP file\n",
        "uploaded = files.upload()  # Select and upload the ZIP file\n",
        "\n",
        "# Assuming there's only one ZIP file uploaded, get its filename\n",
        "zip_filename = next(iter(uploaded.keys()))\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "print(\"Folder structure has been extracted.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "muhkiCOvngjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load data\n",
        "\n",
        "# @markdown If you're working in Colab, go to the files menu by clicking the folder symbol on the left. Right click (or click on the three dots) on the folder you uploaded and choose Copy Path (Pad kopiÃ«ren) and paste below. Check the recursive box if the given directory has subdirectories.\n",
        "\n",
        "directory = '/content/bronnen_in_bytes'#@param {type:\"string\"}\n",
        "\n",
        "reader = SimpleDirectoryReader(\n",
        "    input_dir=f\"{directory}\",\n",
        "    recursive=False # @param {type:\"boolean\"}\n",
        ")\n",
        "\n",
        "all_docs = []\n",
        "\n",
        "for docs in reader.iter_data():\n",
        "    for doc in docs:\n",
        "        doc.text = doc.text.upper()\n",
        "        print(doc.text)\n",
        "        all_docs.append(doc)\n",
        "\n",
        "print(all_docs)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NoSOCpionlx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Chat Away!\n",
        "\n",
        "service_context = ServiceContext.from_defaults(chunk_size=1024,\n",
        "                                               llm=llm,\n",
        "                                               embed_model='local')\n",
        "\n",
        "index = VectorStoreIndex.from_documents(all_docs, service_context=service_context)\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "response_stream = query_engine.query(prompt)\n",
        "response_stream.print_response_stream()"
      ],
      "metadata": {
        "id": "UHsdLh7kqeDD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}